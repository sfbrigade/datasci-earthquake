name: ETL to neon

# Workflow triggers
on:
  schedule:
    - cron: "0 2 * * 0"  # Runs at 2am UTC every Sunday
  workflow_dispatch:  # Allows manual triggering of the workflow

jobs:
  run-etl:
    runs-on: ubuntu-latest  
    outputs:
      changes: ${{ steps.detect_changes.outputs.changes }}    
      etl_commit: ${{ steps.detect_changes.outputs.commit_sha }}
      metadata_table: ${{ steps.export_metadata.outputs.metadata_table }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: develop
          fetch-depth: 0
          ssh-key: ${{ secrets.DEPLOY_KEY }}          

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Get Run ID of Most Recent Successful Run
        id: get_run_id
        run: |
          response=$(curl -s -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
          "https://api.github.com/repos/sfbrigade/datasci-earthquake/actions/workflows/env_vars.yml/runs?status=completed&conclusion=success")
          run_id=$(echo $response | jq '.workflow_runs[0].id')
          echo "Run ID: $run_id"
          echo "run_id=$run_id" >> $GITHUB_ENV     

      - name: Download .env Artifact 
        uses: actions/download-artifact@v4
        with:
          name: encrypted-env-file
          github-token: ${{ secrets.GITHUB_TOKEN }}
          repository: sfbrigade/datasci-earthquake
          run-id: ${{ env.run_id }}    

      - name: Decrypt .env File
        run: |
          openssl aes-256-cbc -d -salt -pbkdf2 -k "${{ secrets.ARTIFACT_PASS }}" -in .env.enc -out .env
          echo "Decryption complete"      

      - name: ETL data to Neon DB
        run: |
          ENVIRONMENT=prod python -m backend.etl.tsunami_data_handler
          ENVIRONMENT=prod python -m backend.etl.soft_story_properties_data_handler
          ENVIRONMENT=prod python -m backend.etl.liquefaction_data_handler

      - name: Query export metadata
        id: export_metadata
        run: |
          cat > query_metadata.py << 'EOF'
          import os
          from sqlalchemy import create_engine
          from backend.api.models.export_metadata import ExportMetadata
          from sqlalchemy.orm import sessionmaker

          engine = create_engine(os.getenv('DATABASE_URL'))
          Session = sessionmaker(bind=engine)
          
          with Session() as session:
              metadata_records = session.query(ExportMetadata).order_by(ExportMetadata.last_exported_at.desc()).all()
              
              if metadata_records:
                  print("| Dataset | Last Exported |")
                  print("|---------|---------------|")
                  for record in metadata_records:
                      formatted_time = record.last_exported_at.strftime('%Y-%m-%d %H:%M:%S UTC')
                      print(f"| {record.dataset_name} | {formatted_time} |")
              else:
                  print("No export metadata found")
          EOF
          
          # Capture the output
          echo "metadata_table<<EOF" >> $GITHUB_OUTPUT
          ENVIRONMENT=prod python query_metadata.py >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Commit & Push Changes
        id: detect_changes 
        run: |
          git config --global user.name "github-actions"
          git config --global user.email "actions@github.com"
          git add public/data/*.geojson
          if git diff --cached --quiet; then 
            echo "No changes to commit"
            echo "changes=false" >> $GITHUB_OUTPUT
          else
            git commit -m "Update datasets (ETL auto-update)"
            echo "changes=true" >> $GITHUB_OUTPUT
            echo "commit_sha=$(git rev-parse HEAD)" >> $GITHUB_OUTPUT
          fi

      - name: Push to develop
        if: steps.detect_changes.outputs.changes == 'true'
        run: |
          git pull --rebase origin develop
          git push origin HEAD:develop


  create-pr:
    needs: run-etl
    if: needs.run-etl.outputs.changes == 'true'
    runs-on: ubuntu-latest
  
    steps:
      - name: Checkout main branch
        uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}
  
      - name: Get updated files from develop
        run: |
          git fetch origin develop
          git checkout origin/develop -- public/data/*.geojson
          
          if ! git diff --quiet; then
            echo "Changes detected:"
            git status --porcelain
          else
            echo "No changes found - this shouldn't happen"
            exit 1
          fi
  
      - name: Create Pull Request
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          branch: etl/update-${{ github.run_id }}
          title: "ETL auto-update - GeoJSON files"
          body: |
            ## ETL Auto-Update
            
            This PR contains updated GeoJSON files from the latest ETL run.
  
            **Source ETL Commit:** ${{ needs.run-etl.outputs.etl_commit }}
            **Workflow Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
            
            ### Export Metadata
            
            ${{ needs.run-etl.outputs.metadata_table }}
            
            Please review and merge if the data looks correct.
          base: main
          delete-branch: true
          commit-message: "ETL: update GeoJSON files"
